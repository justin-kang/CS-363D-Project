{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Darwin Supervised Regression Model Building </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, \n",
    "<br>if you have just received a new api key from support, you will need to register your key and create a new user (see Register user cell)\n",
    "\n",
    "Second, in the Environment Variables cell: \n",
    "1. Set your username and password to ensure that you're able to log in successfully\n",
    "2. Set the path to the location of your datasets if you are using your own data.  The path is set for the examples.\n",
    "3. Set the dataset names accordingly\n",
    "\n",
    "Here are a few things to be mindful of:\n",
    "1. For every run, check the job status (i.e. requested, failed, running, completed) and wait for job to complete before proceeding. \n",
    "2. If you're not satisfied with your model and think that Darwin can benefit from extra training, use the resume function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Darwin SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amb_sdk.sdk import DarwinSdk\n",
    "ds = DarwinSdk()\n",
    "ds.set_url('https://amb-demo-api.sparkcognition.com/v1/');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set your user id and password accordingly\n",
    "USER=\"justin.kang23@gmail.com\"\n",
    "PW=\"psjf52vRYb\"\n",
    "\n",
    "# Set path to datasets - The default below assumes Jupyter was started from amb-sdk/examples/Enterprise/\n",
    "# Modify accordingly if you wish to use your own data\n",
    "PATH_TO_DATASET='data/'\n",
    "DATASET_NAME='data.csv'\n",
    "\n",
    "# A timestamp is used to create a unique name in the event you execute the workflow multiple times or with \n",
    "# different datasets.  File names must be unique in Darwin.\n",
    "import datetime\n",
    "ts = '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from time import sleep\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Login "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status, msg = ds.auth_login_user(USER,PW)\n",
    "if not status:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read dataset and view a file snippet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload dataset to Darwin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_adults(row):\n",
    "    return 100-row['% Age 65 and Older']-row['% Age 17 and Under']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_features(df):\n",
    "    with open('data/remove_col_names.txt', 'r') as f:\n",
    "        drop_cols=f.read().splitlines()\n",
    "    return df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataset\n",
    "df = pd.read_csv(os.path.join(PATH_TO_DATASET, 'exemptions.csv'))\n",
    "df.drop(df.tail(1).index,inplace=True)\n",
    "\n",
    "#combine all data into one dataframe\n",
    "for file in os.listdir(PATH_TO_DATASET):\n",
    "    if 'csv' in file and 'exemptions' not in file: #dont add exemptions twice\n",
    "        df2=pd.read_csv(os.path.join(PATH_TO_DATASET, file))\n",
    "        df=df.merge(df2, on='County')\n",
    "\n",
    "#~feature engineering~\n",
    "df['% Age 18-65']=df.apply (lambda row: calculate_adults(row), axis=1)\n",
    "\n",
    "df=drop_features(df)\n",
    "\n",
    "df.to_csv('pruned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "status, dataset = ds.upload_dataset('pruned_data.csv')\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Data\n",
    "**Before creating a model, users need to analyze data and clean data first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status, analyze_id = ds.analyze_data('pruned_data.csv', \n",
    "                                     job_name = 'Darwin_analyze_data_job' + \"-\" + ts, \n",
    "                                     artifact_name = 'Darwin_analyze_data_artifact' + \"-\" + ts)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job('Darwin_analyze_data_job' + \"-\" + ts)\n",
    "else:\n",
    "    print(analyze_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds.lookup_job_status_name(analyze_id['job_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_name': 'pruned.csv'}\n",
      "{'status': 'Requested', 'starttime': '2019-04-13T19:04:46.412665', 'endtime': None, 'percent_complete': 0, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['pruned_data.csv'], 'artifact_names': ['0c10cffd6e9a408cb54adde9b9dea1ba'], 'model_name': None, 'job_error': None}\n",
      "{'status': 'Complete', 'starttime': '2019-04-13T19:04:46.412665', 'endtime': '2019-04-13T19:04:48.205474', 'percent_complete': 100, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['pruned_data.csv'], 'artifact_names': ['0c10cffd6e9a408cb54adde9b9dea1ba'], 'model_name': None, 'job_error': ''}\n"
     ]
    }
   ],
   "source": [
    "# clean dataset\n",
    "status, job_id = ds.clean_data('pruned_data.csv', impute='ffill')\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " {'filename': '/var/folders/kx/xsyth1zs74b9k_5pl3snlxc00000gn/T/pruned_data.csv-part0-_18n96zn.csv',\n",
       "  'note': 'part 0 of 0',\n",
       "  'part': 0})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.download_dataset('pruned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a model that will learn the class labels in the target column.<br> In the default boston dataset, the target column is \"Assessed_Value\". <br> You will have to specify your own target name for your custom dataset. <br> You can also increase max_train_time for longer training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400: BAD REQUEST - {\"message\": \"Dataset has not been cleaned. Please clean dataset before training model.\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = target + \"_model0\" + ts\n",
    "status, job_id = ds.create_model(dataset_names = 'pruned_data.csv', \\\n",
    "                                 model_name =  model, \\\n",
    "                                 max_train_time = '00:02')\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Training (Optional)\n",
    "Run the following cell for extra training, no need to specify parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404: NOT FOUND - {\"message\": \"Model name not found\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train some more\n",
    "status, job_id = ds.resume_training_model(dataset_names = DATASET_NAME,\n",
    "                                          model_name = model,\n",
    "                                          max_train_time = '00:05')\n",
    "                                          \n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Model\n",
    "Analyze model provides feature importance ranked by the model. <br> It indicates a general view of which features pose a bigger impact on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404: NOT FOUND - {\"message\": \"Model name not found\"}\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-09f61445eb29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'artifact_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# Retrieve feature importance of built model\n",
    "status, artifact = ds.analyze_model(model)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the 10 most important features of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importance[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "**Perform model prediction on the the training dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(DATASET_NAME, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download predictions from Darwin's server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create plots comparing predictions with actual target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot predictions vs actual\n",
    "plt.plot(df[target], prediction[target], '.')\n",
    "plt.plot([0,2.3e7],[0,2.3e7],'--k')\n",
    "print('R^2 : ', r2_score(df[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out which machine learning model did Darwin use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status, model_type = ds.lookup_model_name(model)\n",
    "print(model_type['description']['best_genome'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
